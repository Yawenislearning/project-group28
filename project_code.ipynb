{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c5a136-e872-4846-9965-9ddc5251a0b3",
   "metadata": {},
   "source": [
    "# NYC Apartment Search\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1BYVyFBDcTywdUlanH0ysfOrNWPgl7UkqXA7NeewTzxA/edit#heading=h.bpxu7uvknnbk)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an idea of a possible approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf11fa0-4684-4f5e-8048-0f4cc5f4f243",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a4828",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geoalchemy2 geopandas shapely psycopg2 psycopg2-binary sqlalchemy matplotlib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f675d4b-794e-407c-aac9-b85c4a3975d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import statements needed for the project, for example:\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import urllib.parse\n",
    "import os\n",
    "import geoalchemy2 as gdb\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shapely\n",
    "import sqlalchemy as db\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a62277-51cf-48a2-81d2-9b2127088a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any constants you might need; some have been added for you\n",
    "\n",
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\".\")\n",
    "ZIPCODE_DATA_FILE = pathlib.Path(\"data\")/ \"zipcodes\" / \"nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "NYC_DATA_APP_TOKEN = \"8ITaLVGKJEzelLCfrNyuIi2rJ\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/resource/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "DB_NAME = \"project_database\"\n",
    "DB_USER = \"mac\"\n",
    "DB_URL = f\"postgresql://{DB_USER}@localhost/{DB_NAME}\"\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67cca9-ec72-44e3-83b8-b65f1ed5bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52476a07-9bf2-4b7a-8cb7-93648bb4d303",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b18f12-c0ce-4b9c-adc1-805703edc575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url, jsonname,force=False):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    url_path = parsed_url.path.strip(\"/\")\n",
    "    \n",
    "    filename = DATA_DIR / (url_path.split('/')[-1])\n",
    "    \n",
    "    if force or not filename.exists():\n",
    "        print(f\"Downloading {url} to {jsonname}...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "       \n",
    "        with open(jsonname, \"a\") as f:\n",
    "            json.dump(response.json(), f)\n",
    "        print(f\"Done downloading {url}.\")\n",
    "    else:\n",
    "        print(f\"Reading from {jsonname}...\")\n",
    "\n",
    "    return jsonname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2a5a9-1027-4c41-bbb5-039c32ce7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_311_data(df):\n",
    "    columns = ['unique_key', 'created_date', 'incident_zip', 'complaint_type','longitude','latitude']\n",
    "    df_selected=df[columns]\n",
    "    df_selected=df_selected.dropna(axis=0)\n",
    "    \n",
    "    df_selected.rename(columns={'incident_zip': 'zipcode'}, inplace=True)\n",
    "    df_selected['zipcode']=df_selected['zipcode'].astype(int)\n",
    "    df_selected.rename(columns={'unique_key': 'id'}, inplace=True)\n",
    "    df_selected.rename(columns={'created_date': 'date'}, inplace=True)\n",
    "    df_selected['geometry']=gpd.points_from_xy(df_selected['longitude'],df_selected['latitude'],crs=\"EPSG:4326\")\n",
    "    df_selected=df_selected.drop(['latitude','longitude'],axis=1)\n",
    "    df_selected['date'] = pd.to_datetime(df_selected['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return gpd.GeoDataFrame(df_selected,geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_311_data(offset):\n",
    "    base_url = f\"{BASE_NYC_DATA_URL}{NYC_DATA_311}?$$app_token={NYC_DATA_APP_TOKEN}\"\n",
    "    url = f'{base_url}&$limit=1000000&$offset={offset}&$where=created_date between\"2018-01-01T12:00:00.000\"and\"2023-09-30T12:00:00.000\"'\n",
    "    download_nyc_geojson_data(url,jsonname=f'query_{offset/1000000+1}_data.geojson',force=False)\n",
    "    print('begin to create csv')\n",
    "    df=gpd.read_file(f'query_{offset/1000000+1}_data.geojson')\n",
    "    print('finishing create csv')\n",
    "    df.to_csv(f'query_{offset/1000000+1}_data.csv', index=False)\n",
    "    return clean_311_data(pd.read_csv(f'query_{offset/1000000+1}_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa407bd",
   "metadata": {},
   "source": [
    "### zipcode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4005f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile):\n",
    "    gdf = gpd.read_file(zipcode_datafile)\n",
    "    #normalized data\n",
    "    gdf_normalized = gdf.to_crs(epsg=4326)\n",
    "    \n",
    "    #selected_zipcode\n",
    "    columns=['ZIPCODE', 'PO_NAME', 'STATE','COUNTY','geometry']\n",
    "    gdf_selected=gdf_normalized[columns]\n",
    "    gdf_selected.rename(columns={'ZIPCODE': 'zipcode'}, inplace=True)\n",
    "    gdf_selected.rename(columns={'COUNTY': 'county'}, inplace=True)\n",
    "    gdf_selected.rename(columns={'STATE': 'state'}, inplace=True)\n",
    "    gdf_selected.rename(columns={'PO_NAME': 'poname'}, inplace=True)\n",
    "    #cleaned_zipcode\n",
    "    gdf_cleaned = gdf_selected.dropna()\n",
    "    return gdf_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ef4b9",
   "metadata": {},
   "source": [
    "### tree data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde1b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_tree_data():\n",
    "    base_url = f\"{BASE_NYC_DATA_URL}{NYC_DATA_TREES}?$$app_token={NYC_DATA_APP_TOKEN}\"\n",
    "    url = f'{base_url}&$limit=1000000'\n",
    "    download_nyc_geojson_data(url,jsonname='nyc_tree.geojson',force=False)\n",
    "    df = gpd.read_file('nyc_tree.geojson')\n",
    "    df.to_csv('nyc_tree.csv',index=False)\n",
    "    \n",
    "    df_read=pd.read_csv(\"nyc_tree.csv\")\n",
    "    columns = ['tree_id','zipcode','longitude', 'latitude','spc_common','health','status']\n",
    "    df_selected = df_read[columns]\n",
    "    df_selected.rename(columns={'tree_id': 'id'}, inplace=True)\n",
    "    df_selected['geometry']=gpd.points_from_xy(df_selected['longitude'],df_selected['latitude'],crs=\"EPSG:4326\")\n",
    "    \n",
    "    df_drop=df_selected.drop(['latitude','longitude'],axis=1)\n",
    "    df_cleaned = df_drop.dropna()\n",
    "    return gpd.GeoDataFrame(df_cleaned,geometry='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340db03e",
   "metadata": {},
   "source": [
    "### zillow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efddc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data(ZILLOW_DATA_FILE):\n",
    "    df = pd.read_csv(ZILLOW_DATA_FILE)\n",
    "    # only choose NY data\n",
    "    filtered_df = df[df['State'] == 'NY']\n",
    "    #change data from wide to long\n",
    "    df_melt=filtered_df.melt(id_vars=['RegionName'],value_vars=['2015-01-31', '2015-02-28', '2015-03-31', '2015-04-30', '2015-05-31',\n",
    "       '2015-06-30', '2015-07-31', '2015-08-31', '2015-09-30', '2015-10-31','2015-11-30',\n",
    "       '2015-12-31', '2016-01-31', '2016-02-29', '2016-03-31', '2016-04-30',\n",
    "       '2016-05-31', '2016-06-30', '2016-07-31', '2016-08-31', '2016-09-30',\n",
    "       '2016-10-31', '2016-11-30', '2016-12-31', '2017-01-31', '2017-02-28',\n",
    "       '2017-03-31', '2017-04-30', '2017-05-31', '2017-06-30', '2017-07-31',\n",
    "       '2017-08-31', '2017-09-30', '2017-10-31', '2017-11-30', '2017-12-31',\n",
    "       '2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30', '2018-05-31',\n",
    "       '2018-06-30', '2018-07-31', '2018-08-31', '2018-09-30', '2018-10-31',\n",
    "       '2018-11-30', '2018-12-31', '2019-01-31', '2019-02-28', '2019-03-31',\n",
    "       '2019-04-30', '2019-05-31', '2019-06-30', '2019-07-31', '2019-08-31',\n",
    "       '2019-09-30', '2019-10-31', '2019-11-30', '2019-12-31', '2020-01-31',\n",
    "       '2020-02-29', '2020-03-31', '2020-04-30', '2020-05-31', '2020-06-30',\n",
    "       '2020-07-31', '2020-08-31', '2020-09-30', '2020-10-31', '2020-11-30',\n",
    "       '2020-12-31', '2021-01-31', '2021-02-28', '2021-03-31', '2021-04-30',\n",
    "       '2021-05-31', '2021-06-30', '2021-07-31', '2021-08-31', '2021-09-30',\n",
    "       '2021-10-31', '2021-11-30', '2021-12-31', '2022-01-31', '2022-02-28',\n",
    "       '2022-03-31', '2022-04-30', '2022-05-31', '2022-06-30', '2022-07-31',\n",
    "       '2022-08-31', '2022-09-30', '2022-10-31', '2022-11-30', '2022-12-31',\n",
    "       '2023-01-31', '2023-02-28', '2023-03-31', '2023-04-30', '2023-05-31',\n",
    "       '2023-06-30', '2023-07-31', '2023-08-31', '2023-09-30'],var_name='date',value_name='rent')\n",
    "    df_cleaned = df_melt.dropna()\n",
    "    df_cleaned.rename(columns={'RegionName': 'zipcode'}, inplace=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dd5b3",
   "metadata": {},
   "source": [
    "### load all data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed57a37",
   "metadata": {},
   "source": [
    "#### nyc_311 data has download limitation, so we split it into 25 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=download_311_data(1*1000000)\n",
    "df2=download_311_data(2*1000000)\n",
    "df3=download_311_data(3*1000000)\n",
    "df4=download_311_data(4*1000000)\n",
    "df5=download_311_data(5*1000000)\n",
    "df6=download_311_data(6*1000000)\n",
    "df7=download_311_data(7*1000000)\n",
    "df8=download_311_data(8*1000000)\n",
    "df9=download_311_data(9*1000000)\n",
    "df10=download_311_data(10*1000000)\n",
    "df11=download_311_data(11*1000000)\n",
    "df12=download_311_data(12*1000000)\n",
    "df13=download_311_data(13*1000000)\n",
    "df14=download_311_data(14*1000000)\n",
    "df15=download_311_data(15*1000000)\n",
    "df16=download_311_data(16*1000000)\n",
    "df17=download_311_data(17*1000000)\n",
    "df18=download_311_data(18*1000000)\n",
    "df19=download_311_data(19*1000000)\n",
    "df20=download_311_data(20*1000000)\n",
    "df21=download_311_data(21*1000000)\n",
    "df22=download_311_data(22*1000000)\n",
    "df23=download_311_data(23*1000000)\n",
    "df24=download_311_data(24*1000000)\n",
    "df25=download_311_data(25*1000000)\n",
    "df26=download_311_data(26*1000000)\n",
    "df27=download_311_data(27*1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df1=pd.read_csv(pathlib.Path(\"data-311\")/'query_1_data.csv')\n",
    "df2=pd.read_csv(pathlib.Path(\"data-311\")/'query_2_data.csv')\n",
    "df3=pd.read_csv(pathlib.Path(\"data-311\")/'query_3_data.csv')\n",
    "df4=pd.read_csv(pathlib.Path(\"data-311\")/'query_4_data.csv')\n",
    "df5=pd.read_csv(pathlib.Path(\"data-311\")/'query_5_data.csv')\n",
    "df6=pd.read_csv(pathlib.Path(\"data-311\")/'query_6_data.csv')\n",
    "df7=pd.read_csv(pathlib.Path(\"data-311\")/'query_7_data.csv')\n",
    "df8=pd.read_csv(pathlib.Path(\"data-311\")/'query_8_data.csv')\n",
    "df9=pd.read_csv(pathlib.Path(\"data-311\")/'query_9_data.csv')\n",
    "df10=pd.read_csv(pathlib.Path(\"data-311\")/'query_10_data.csv')\n",
    "df11=pd.read_csv(pathlib.Path(\"data-311\")/'query_11_data.csv')\n",
    "df12=pd.read_csv(pathlib.Path(\"data-311\")/'query_12_data.csv')\n",
    "df13=pd.read_csv(pathlib.Path(\"data-311\")/'query_13_data.csv')\n",
    "df14=pd.read_csv(pathlib.Path(\"data-311\")/'query_14_data.csv')\n",
    "df15=pd.read_csv(pathlib.Path(\"data-311\")/'query_15_data.csv')\n",
    "df16=pd.read_csv(pathlib.Path(\"data-311\")/'query_16_data.csv')\n",
    "df17=pd.read_csv(pathlib.Path(\"data-311\")/'query_17_data.csv')\n",
    "df18=pd.read_csv(pathlib.Path(\"data-311\")/'query_18_data.csv')\n",
    "df19=pd.read_csv(pathlib.Path(\"data-311\")/'query_19_data.csv')\n",
    "df20=pd.read_csv(pathlib.Path(\"data-311\")/'query_20_data.csv')\n",
    "df21=pd.read_csv(pathlib.Path(\"data-311\")/'query_21_data.csv')\n",
    "df22=pd.read_csv(pathlib.Path(\"data-311\")/'query_22_data.csv')\n",
    "df23=pd.read_csv(pathlib.Path(\"data-311\")/'query_23_data.csv')\n",
    "df24=pd.read_csv(pathlib.Path(\"data-311\")/'query_24_data.csv')\n",
    "df25=pd.read_csv(pathlib.Path(\"data-311\")/'query_25_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638eb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the data into one dataframe\n",
    "df_311=pd.concat([df1, df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,df21,df22,df23,df24,df25], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345ebc2c-14f1-490c-9857-11f1e332e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(df):\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    geodf_311_data = clean_311_data(df)\n",
    "    geodf_tree_data = load_and_clean_tree_data()\n",
    "    geodf_zillow_data = load_and_clean_zillow_data(ZILLOW_DATA_FILE)\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        geodf_zillow_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2768bc8-4130-4298-be28-13d4b250a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, geodf_zillow_data = load_all_data(df_311)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49c4b5",
   "metadata": {},
   "source": [
    "### show dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad8bbc-bf91-457e-97db-a945fabeee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info about each dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68f4be-f365-46c1-91a1-ab75deb75ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 entries about each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a803b68-2f07-44b8-8b24-d4f16c9e03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14705df9-ea77-4d57-ac8e-1845f80a216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6006cd2-3a00-4660-8d2a-a660b9bfd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f880ef-c5fc-4159-8174-21ccd44f492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59724f74-5f1e-435c-b843-f381a875dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3373556",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zillow_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685942c-26dc-40db-84c2-a71aa3340806",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe158af",
   "metadata": {},
   "source": [
    "### Creating databese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ed80d-7b60-484f-a123-23b673b0f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "connection = psycopg2.connect(dbname=\"postgres\",user=DB_USER,host=\"localhost\",port=\"5432\" )\n",
    "connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"CREATE DATABASE {}\".format(DB_NAME))\n",
    "print(f\"create {DB_NAME} sucessfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca11ce4",
   "metadata": {},
   "source": [
    "### Creating extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection1 = psycopg2.connect(dbname=DB_NAME,user=DB_USER,host=\"localhost\",port=\"5432\" )\n",
    "cursor1 = connection1.cursor()\n",
    "cursor1.execute('CREATE EXTENSION postgis;')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a251c-f337-4b24-bb41-96ee4621a9bd",
   "metadata": {},
   "source": [
    "### Creating Tables\n",
    "\n",
    "\n",
    "These are just a couple of options to creating your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d0cc6-74b3-4d35-a454-57f647c9f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the SQL statements to create your 4 tables\n",
    "ZIPCODE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc_zipcode(\n",
    "    zipcode INTEGER ,\n",
    "    poname VARCHAR,\n",
    "    state VARCHAR,\n",
    "    county VARCHAR,\n",
    "    geometry geometry(Polygon, 4326)\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "NYC_311_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc_311(\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    date DATE,\n",
    "    zipcode INTEGER,\n",
    "    complaint_type VARCHAR,\n",
    "    geometry GEOMETRY(Point, 4326)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "NYC_TREE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc_tree(\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    zipcode INTEGER,\n",
    "    spc_common VARCHAR,\n",
    "    health VARCHAR,\n",
    "    status VARCHAR,\n",
    "    geometry GEOMETRY(Point, 4326)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "ZILLOW_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS zillow(\n",
    "    zipcode INTEGER,\n",
    "    date DATE,\n",
    "    rent DECIMAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "indance='''\n",
    "CREATE INDEX idx_geometries_geom_gist ON zipcode USING gist (geometry)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d86f6-ff6e-4bb8-8fa2-df0d4282e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(\"schema.sql\", \"w\") as f:\n",
    "    f.write(ZIPCODE_SCHEMA)\n",
    "   \n",
    "    f.write(NYC_311_SCHEMA)\n",
    "  \n",
    "    f.write(NYC_TREE_SCHEMA)\n",
    "\n",
    "    f.write(ZILLOW_SCHEMA)\n",
    "    \n",
    "    f.write(indance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schema.sql\", 'r') as file:\n",
    "    sql_commands = file.read()\n",
    "commands = sql_commands.split(';')\n",
    "\n",
    "for command in commands:\n",
    "    if command.strip(): \n",
    "        print(command)\n",
    "        connection2 = psycopg2.connect(dbname=DB_NAME,user=DB_USER,host=\"localhost\",port=\"5432\" )\n",
    "        cursor2 = connection2.cursor()\n",
    "        cursor2.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b97bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e88a50c-9528-4a5c-9a52-b96781ee8985",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "These are just a couple of options to write data to your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e37800-cd95-44b5-9c21-eb7ac2b2e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(DB_USER,DB_NAME):\n",
    "    engine = create_engine(f\"postgresql+psycopg2://{DB_USER}@localhost/{DB_NAME}\")\n",
    "    geodf_zipcode_data.to_postgis('zipcode', engine, if_exists='append', index=False)\n",
    "    geodf_311_data.to_postgis('nyc_311', engine, if_exists='append', index=False)\n",
    "    geodf_tree_data.to_postgis('nyc_tree', engine, if_exists='append', index=False)\n",
    "    geodf_zillow_data.to_sql('zillow', engine, if_exists='append', index=False)\n",
    "    \n",
    "write_dataframes_to_table(DB_USER,DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63b553-0c64-4da8-9fc7-41555d89d853",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7e12b-e251-4f08-8dc5-601db30c2089",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "Query 1: Which area might be more calm to live in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce8548-4aba-4bf9-992c-dedd0f249db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(QUERY_1, QUERY_1_FILENAME):\n",
    "    with open (QUERY_1_FILENAME,'w')as f:\n",
    "        f.write(QUERY_1)\n",
    "        f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605e6f3-ec42-4a8b-833c-5138c14b678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = QUERY_DIR / \"query1\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce044adf-ecdf-4237-9b20-b7cdaaab0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_1))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b6daf",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "Query 2: Where has the most greenery?\n",
    "Zipcode\n",
    "tree_id\n",
    "Using just the trees table, which 10 zip codes have the most trees?\n",
    "\n",
    "The query result should have two columns, 10 rows. The rows should be sorted by the total number of trees, descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = QUERY_DIR / \"query2\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee45766",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_2))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101325e6",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "\n",
    "Query 3: Can I afford a place in the areas with the most trees?\n",
    "Zillow_rent\n",
    "RegionName, 2023-08-31\n",
    "\n",
    "\n",
    "Of the 10 zip codes with the most trees, for the month of August 2023, what is the average rent by zip code?\n",
    "\n",
    "The query should have a JOIN statement. The query result should have two columns (not three) and 10 rows. The rows should be sorted by the total number of trees, descending. “Humanize” the rent numbers, meaning format the results as 2,879.58 instead of 2879.575128. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = QUERY_DIR / \"query3\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb121a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_3))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40630d",
   "metadata": {},
   "source": [
    "### Query4\n",
    "Query 4: Could there be a correlation between an area’s rent, the number of its trees, and the number of 311 complaints?\n",
    "2023-1\n",
    "For the month of January 2023, return the 5 zip codes with the lowest average rent, and 5 zipcodes of the highest average rent, and include the tree count and complaint count for each zip code by using JOIN statements.\n",
    "\n",
    "The query result should have 4 columns (zip code, average rent, tree count, and complaint count) and 10 rows: five with the highest average rent, and five with the lowest average rent. “Humanize” the rent numbers, meaning format the results as 2,879.58 instead of 2879.575128.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = QUERY_DIR / \"query4\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_4))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6dcd3",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "\n",
    "Query 5: Where has the most greenery (take 2)?\n",
    "Rewrite Query 2 to use both the trees table and the zipcodes table. Join both tables where the coordinate point of the tree is inside the polygon boundary of the zipcode as defined in the zipcode table.\n",
    "\n",
    "\n",
    "\n",
    "The query should have a JOIN statement. The query results should match exactly the results of Query 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765aed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = QUERY_DIR / \"query5\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_5))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b08bde",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "\n",
    "Using the following coordinate pair on campus, which trees are within ½ mile radius of this\n",
    "point?\n",
    "Latitude: 40.80737875669467, Longitude: -73.96253174434912\n",
    "Tree: longitude, latitude, tree_id, spc_common, health, status, x_sp, y_sp\n",
    "The result should have 5 columns (ID, species, health, status, and coordinate location of each\n",
    "tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1071f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = QUERY_DIR / \"query6\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ad7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_6))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75223ce5-6ab5-4613-b6af-fa8e33bcc7d5",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fcfed-ddbb-4908-a60e-ed7cbc6d5b00",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e2cde-e43b-407b-ab93-ff85a2dba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80755f-d1e1-4e53-8ef8-f5295c59a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query your database for the data needed.\n",
    "    # You can put the data queried into a pandas/geopandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2632a-b516-4a6e-8b67-97116ab6fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
